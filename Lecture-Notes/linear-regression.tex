\chapter{Linear Regression}
A great deal of the current success of artificial intelligence is due to recent advances in
\href{https://en.wikipedia.org/wiki/Machine_learning}{machine learning}.  
In order to get a first taste of what machine learning is about, we introduce 
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear regression} in this chapter, since linear regression
is one of the most basic algorithms in machine learning.  It is also the foundation for more advanced
forms of machine learning like \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} and 
\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{neural networks}.
Furthermore, linear regression is surprisingly powerful.  Finally, many of the fundamental problems of machine
learning can already be illustrated with linear regression.  Therefore it is only natural that we begin our
study of machine learning with the study of linear regression.

\section{Simple Linear Regression}
Assume we want to know how the \href{https://en.wikipedia.org/wiki/Engine_displacement}{engine displacement} of
a car engine relates to the fuel consumption of the car.  Of course, we could try to create a theoretical model that
relates the engine displacement to its fuel consumption.  However, due to our lack of understanding of engine
physics, this is not an option for us.  Instead, we could try to take a look at different engines and compare
their engine displacement with the corresponding fuel consumption.  This way, we would collect a set of  
$m$ observations of the form $\langle x_1, y_1\rangle, \cdots, \langle x_m, y_m\rangle$ 
where $x_i$ is the engine displacement of the engine in the $i$-th car, while $y_i$ is the fuel consumption of the
$i$-th car.  We call $x$ is the \emph{\color{blue}independent variables}, while $y$ is the 
\emph{\color{blue}dependent variable}.  We define the vectors $\mathbf{x}$ and $\mathbf{y}$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x} := \langle x_1, \cdots, x_m \rangle^{\mathrm{T}}$ \quad and \quad
$\mathbf{y} := \langle y_1, \cdots, y_m \rangle^{\mathrm{T}}$.
\\[0.2cm]
Here, the operator $^\mathrm{T}$ is interpreted as the \href{https://en.wikipedia.org/wiki/Transpose}{transpose} operator,
i.e.~$\mathbf{x}$ and $\mathbf{y}$ are considered to be column vectors.

In linear regression, we use a \emph{\color{blue}linear hypothesis} 
and assume that the dependent variable $y_i$ is related to the independent variable $x_i$ via a linear
equation of the form
\\[0.2cm]
\hspace*{1.3cm}
$y_i := \vartheta_1 \cdot x_i + \vartheta_0$.
\\[0.2cm]
We do not expect this equation to hold exactly.  The reason is that there are many other factors besides the
engine displacement that influence the fuel consumption.  For example, both the weight of a car and its 
\href{https://en.wikipedia.org/wiki/Automotive_aerodynamics}{aerodynamics} certainly influence the fuel consumption.  
We want to calculate those values $\vartheta_0$ and $\vartheta_1$ such that the 
\emph{\color{blue}\underline{m}ean \underline{s}quared \underline{e}rror}, which is defined as 
\begin{equation}
  \label{eq:mse}
 \mathtt{MSE}(\vartheta_0, \vartheta_1) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2,
\end{equation}
is minimized.  It can be shown that the solution to this minimization problem is given as follows:
\begin{equation}
  \label{eq:theta0}
   \vartheta_1 = r_{x,y} \cdot \frac{s_y}{s_x} \quad \mbox{and} \quad \ds\vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}.
\end{equation}
This solution makes use of the values $r_{x,y}$, $s_x$, and $s_y$.  In order to define these values, we first
define the \emph{\color{blue}sample mean values} $\bar{\mathbf{x}}$ and $\bar{\mathbf{y}}$ of $\mathbf{x}$ and $\mathbf{y}$ respectively, i.e.~we have 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \bar{\mathbf{x}} = \frac{1}{m} \cdot \sum\limits_{i=1}^m x_i$ \quad and \quad
$\ds \bar{\mathbf{y}} = \frac{1}{m} \cdot \sum\limits_{i=1}^m y_i$.
\\[0.2cm]
Furthermore, $s_x$ and $s_y$ are the \emph{\color{blue}sample standard deviations} of $\mathbf{x}$ and $y$, i.e.~we have
\\[0.2cm]
\hspace*{1.3cm}
$\ds s_x = \sqrt{\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2\;}$ \quad and \quad
$\ds s_y = \sqrt{\frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2\;}$.
\\[0.2cm]
Finally, $r_{x,y}$ is the \emph{\color{blue}sample correlation coefficient} that is defined as
\\[0.2cm]
\hspace*{1.3cm}
$\ds r_{x,y} = \frac{1}{(m-1) \cdot s_x \cdot s_y} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)$.
\\[0.2cm]
The number $r_{x,y}$ is also known as the
\href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Pearson correlation coefficient} or
\blue{Pearson's \textrm{r}}.  It is named after \href{https://en.wikipedia.org/wiki/Karl_Pearson}{Karl Pearson}
(1857 -- 1936).
Note that the formula for the parameter $\vartheta_1$ can be simplified to  
\begin{equation}
  \label{eq:theta1}
\ds\vartheta_1 = \frac{\sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)}{
                        \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2}  
\end{equation}
This latter formula should be used to calculate $\vartheta_1$.  However, the previous formula is also useful
because it shows the the correlation coefficient is identical to the coefficient $\vartheta_1$, provided the variables $\mathbf{x}$ and
$\mathbf{y}$ have been normalized so that their standard deviation is $1$.

\exercise
Proof Equation \ref{eq:theta0} and Equation \ref{eq:theta1}.

\hint
Take the partial derivatives of $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ with respect to $\vartheta_0$ and
$\vartheta_1$.  In order to minimize the expression  $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ with respect to
$\vartheta_0$ and $\vartheta_1$, these derivatives have to be set to $0$.
\eox

\subsection{Accessing the Quality of Linear Regression}
Assume that we have been given a set of $m$ observations of the form $\langle x_1, y_1\rangle, \cdots, \langle x_m, y_m\rangle$  
and that we have calculated the parameters $\vartheta_0$ and $\vartheta_1$ according to Equation
\ref{eq:theta0} and Equation \ref{eq:theta1}.  Provided that not all $x_i$ have the same value, these
formul\ae\ will return two numbers for $\vartheta_0$ and $\vartheta_1$ hat define a linear model for
$\mathbf{y}$ in terms of $\mathbf{x}$.  However, at the moment we still lack a number that tell us how good
this linear model really is.  In order to judge the quality of the linear model given by 
\\[0.2cm]
\hspace*{1.3cm}
$y = \vartheta_0 + \vartheta_1 \cdot x$
\\[0.2cm]
we can compute the mean squared error according to Equation \ref{eq:mse}.  However, the mean squared error 
is an absolute number that, by itself, is difficult to interpret.  The reason is that the variable $\mathbf{y}$ might be
inherently noisy and we have to relate this noise to the mean squared error.  Now the noise contained in $\mathbf{y}$
can be measured by the \blue{sample variance} of $\mathbf{y}$ and is given by the formula
\begin{equation}
  \label{eq:var}
  \mathtt{Var}(y) := \frac{1}{m-1} \cdot \sum\limits_{i=1}^m \bigl(y_i - \bar{y}\bigr)^2.
\end{equation}
If we compare this formula to the formula for the mean squared error
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{MSE}(\vartheta_0, \vartheta_1) := 
  \frac{1}{m-1} \cdot\sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
$,
\\[0.2cm]
we see that the sample variance of $\mathbf{y}$ is an upper bound for the mean squared error since we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{Var}(\mathbf{y}) = \mathtt{MSE}(\bar{\mathbf{y}}, 0)$,
\\[0.2cm]
i.e.~the sample variance is the value that we would get for the mean squared error if we would set $\vartheta_0$ to
the average value of $\mathbf{y}$ and $\vartheta_1$ to zero.  Since $\vartheta_0$ and $\vartheta_1$ are chosen to
minimize the mean squared error, we have
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{MSE}(\vartheta_0, \vartheta_1) \leq \mathtt{MSE}(\bar{\mathbf{y}}, 0) = \mathtt{Var}(\mathbf{y})$.
\\[0.2cm]
The mean squared error is an absolute value and, therefore, difficult to interpret.  The fraction
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{\mathtt{MSE}(\vartheta_0, \vartheta_1)}{\mathtt{Var}(y)}$
\\[0.2cm]
is called the proportion of the \emph{\color{blue}unexplained variance} because it is the variance that is still
left if we use our linear model to predict the values of $\mathbf{y}$ given the values of $\mathbf{x}$.  The
\emph{\color{blue}explained variance} which is also known as the \emph{\color{blue}$\mathtt{R}^2$ statistic} is defined as 
\begin{equation}
  \label{eq:Rsquare}
  \mathtt{R}^2 := \frac{\mathtt{Var}(\mathbf{y}) - \mathtt{MSE}}{\mathtt{Var}(\mathbf{y})} = 1 - \frac{\mathtt{MSE}}{\mathtt{Var}(\mathbf{y})}.
\end{equation}
Here $\mathtt{MSE}$ is short for $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ where we have substituted the values
from equation \ref{eq:theta0} and \ref{eq:theta1}.  The $\mathtt{R}^2$ statistic measures the quality of our
model: If it is small, then our model does not explain the variation of the value of $\mathbf{y}$ when the value of $\mathbf{x}$
changes.  On the other hand, if it is near a $100\%$, then our model does a good job in explaining the 
variation of $\mathbf{y}$ when $\mathbf{x}$ changes.

Since the formul\ae\ for $\mathtt{Var}(\mathbf{y})$ and $\mathtt{MSE}(\vartheta_0, \vartheta_1)$ have the same
denominator $m-1$, this denominator can be cancelled when the $\mathtt{R}^2$ statistic is computed.  To this
end we define the \emph{\color{blue}total sum of squares} $\mathtt{TSS}$ as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{TSS} := \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2 = (m-1) \cdot \mathtt{Var}(\mathbf{y})$
\\[0.2cm]
and the \emph{\color{blue}residual sum of squares} $\mathtt{RSS}$ as
\\[0.2cm]
\hspace*{1.3cm}
$\ds\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2
                  = (m-1) \cdot \mathtt{MSE}(\vartheta_0, \vartheta_1)
$.
\\[0.2cm]
Then the formula for the $\mathtt{R}^2$ statistic can be written as
\\[0.2cm]
\hspace*{1.3cm}
$\ds \mathtt{R}^2 = 1 - \frac{\mathtt{RSS}}{\mathtt{TSS}}$.
\\[0.2cm]
This is the formula that we will use when we implement simple linear regression.

It should be noted that $\mathrm{R}^2$ is the square of Pearson's \textrm{r}.  The notation is a bit
inconsistent since Pearson's $r$ is written in lower case, while $\mathrm{R}^2$ is written in upper
case.  Finally $\mathrm{R}^2$ is also known as the 
\href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{coefficient of determination}.  It tells us
to what is extend the value of the variable $y$ is determined by the value of $x$.


\subsection{Putting the Theory to the Test}
In order to get a better feeling for linear regression, we want to test it to investigate the factors that
determine the fuel consumption of cars.  \myFig{cars.csv} shows the head of the data file ``\texttt{cars.csv}''
which I have adapted from the file
\\[0.2cm]
\hspace*{1.3cm}
\href{http://www-bcf.usc.edu/~gareth/ISL/Auto.csv}{\texttt{http://www-bcf.usc.edu/\symbol{126}gareth/ISL/Auto.csv}}.
\\[0.2cm]
\myFig{cars.csv} shows the column headers and the first ten data entries contained in this file.  
Altogether, this file contains data of 392 different car models.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
     mpg, cyl, displacement,    hp, weight,  acc, year, name
    18.0,   8,        307.0, 130.0, 3504.0, 12.0,   70, chevrolet chevelle malibu
    15.0,   8,        350.0, 165.0, 3693.0, 11.5,   70, buick skylark 320
    18.0,   8,        318.0, 150.0, 3436.0, 11.0,   70, plymouth satellite
    16.0,   8,        304.0, 150.0, 3433.0, 12.0,   70, amc rebel sst
    17.0,   8,        302.0, 140.0, 3449.0, 10.5,   70, ford torino
    15.0,   8,        429.0, 198.0, 4341.0, 10.0,   70, ford galaxie 500
    14.0,   8,        454.0, 220.0, 4354.0,  9.0,   70, chevrolet impala
    14.0,   8,        440.0, 215.0, 4312.0,  8.5,   70, plymouth fury iii
    14.0,   8,        455.0, 225.0, 4425.0, 10.0,   70, pontiac catalina
    15.0,   8,        390.0, 190.0, 3850.0,  8.5,   70, amc ambassador dpl
\end{Verbatim}
\vspace*{-0.3cm}
\caption{The head of the file \texttt{cars.csv}.}
\label{fig:cars.csv}
\end{figure}

The file ``\texttt{cars.csv}'' is part of the data set accompanying the excellent book 
\href{http://www-bcf.usc.edu/~gareth/ISL/index.html}{Introduction to Statistical Learning} by Gareth James,
Daniela Witten, Trevor Hastie, and Robert Tibshirani \cite{james:2014}.  The file 
``\texttt{cars.csv}''  contains the fuel consumption of a number of different cars that were in widespread use during
the seventies and early eighties of the last century.  The first column of this data set list the 
\emph{\color{blue}miles per gallon}, i.e.~the number of miles a car can go with one gallon of gas.  Note that
this number is inverse to the fuel consumption:  If a car $\mathrm{A}$ can go twice as many miles per gallon
than another car $\mathrm{B}$, then the fuel consumption of $\mathrm{A}$ is half of the fuel consumption of
$\mathrm{B}$. Furthermore, besides the miles per gallon, for every car the following other parameters are listed:
\begin{enumerate}
\item $\mathtt{cyl}$ is the number of cylinders,
\item $\mathtt{displacement}$ is the engine displacement in cubic inches, 
\item $\mathtt{hp}$ is the engine power given in units of \href{https://en.wikipedia.org/wiki/Horsepower}{horsepower},
\item $\mathtt{weight}$ is the weight in pounds,
\item $\mathtt{acc}$ is the acceleration given as the time in seconds needed to accelerate from 0 miles per
      hour to 60 miles per hour,
\item $\mathtt{year}$ is the year in which the model was introduced, and
\item $\mathtt{name}$ is the name of the model.
\end{enumerate}
Our aim is to determine which of these parameters can be used best to explain the fuel consumption of a car.  To this
end, I have written the program shown in \myFig{simple-linear-regression.stlx}.


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    simple_linear_regression := procedure(fileName, types, name) {
        csv    := readTable(fileName, types);
        data   := csv.getData();
        number := #data;
        index  := find_index(name, csv.getColumnNames());
        Y      := [];
        X      := [];
        L      := [1 .. number];
        for (i in L) {
            Y[i] := 1 / data[i][1];
            X[i] := data[i][index];
        }
        xMean  := +/ X / number;
        yMean  := +/ Y / number;
        theta1 :=   (+/ [(X[i]-xMean)*(Y[i]-yMean) : i in L]) 
                  / (+/ [(X[i]-xMean)**2 : i in L]);
        theta0 := yMean - theta1 * xMean;
        TSS    := +/ [(Y[i] - yMean) ** 2 : i in L];
        RSS    := +/ [(Y[i] - theta0 - theta1 * X[i]) ** 2 : i in L];
        R2     := 1 - RSS / TSS;
        canvas := plot_createCanvas("Fuel consumption vs. $name$");
        plot_addBullets(canvas, [ [X[i], Y[i]] : i in L ], [0,0,255], 2.0);
        return R2;
    };
    find_index := procedure(x, List) {
        return arb({ idx : idx in [1 .. #List] | List[idx] == x });
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Simple Linear Regression}
\label{fig:simple-linear-regression.stlx}
\end{figure}


\noindent
The procedure $\mathtt{simple\_linear\_regression}$ take three arguments:
\begin{enumerate}[(a)]
\item $\mathtt{fileName}$ is the name of the file containing the data.

      It is assumed that this file is a \texttt{csv}-file containing the data that need to be analyzed.
\item $\mathtt{types}$ is a list of type names of the columns present in the \texttt{csv} file.  This is needed
      by \textsc{SetlX} in order to correctly interpret the data.

\item $\mathtt{name}$ is the column name that is to be used for linear regression, i.e.~it specifies the column that 
      contains the values for the independent variable $x$.  The dependent variable $y$ is assumed to be given in
      the first column of the \texttt{csv}-file.
\end{enumerate}
The implementation of the procedure $\mathtt{simple\_linear\_regression}$ works as follows:
\begin{enumerate}
\item The function \texttt{readTable} is used to read the file specified in $\mathtt{fileName}$.
      It returns the object $\mathtt{csv}$ which is of class $\mathtt{Table}$.  This class and the implementation of the function
      $\mathtt{readTable}$ is shown in \myFig{table.stlx}.  The object $\mathtt{csv}$ contains both the column
      names as well as the data that is present in the given file.  The list $\mathtt{types}$ is needed in
      order to convert the strings that are read into their proper data types.
\item The function $\mathtt{getData}$ extracts the $\mathtt{data}$ from the object $\mathtt{csv}$.
      Technically, $\mathtt{data}$ is a list of lists.  The list $\mathtt{data}$ has the same length as there are lines
      in the file specified by $\mathtt{fileName}$.  Every single line in this file is converted
      into a list of entries of the appropriate types.  These lists are the elements of the list $\mathtt{data}$.
\item $\mathtt{number}$ is the number of data lines in the file specified by $\mathtt{fileName}$,
      i.e.~$\mathtt{number}$ is what we have called $m$ in the formul\ae\ given previously.
\item $\mathtt{index}$ is the index of the column that has the given $\mathtt{name}$.
      For example, in the case of the file shown in \myFig{cars.csv}, the $\mathtt{index}$ of the
      $\mathtt{name}$ ``$\mathtt{displacement}$'' is $3$ as the engine displacement is given in the third
      column of the file ``\texttt{cars.csv}''.
\item The values of the dependent variable $y$ are stored in the list $\mathtt{Y}$ and the values of the
      independent variable $x$ are stored in the list $\mathtt{X}$.
\item The list $\mathtt{L}$ is used as an abbreviation so that iterations over all lists stored in
      $\mathtt{data}$ are simplified.
\item The \texttt{for}-loop fills the lists $\mathtt{X}$ and $\mathtt{Y}$.  Since the file
      ``\texttt{cars.csv}'' contains the miles per gallon in the first column labelled $\mathtt{mpg}$,
      we need to convert this data into fuel consumption.  As the fuel consumption is the reciprocal of the miles per
      gallon, the list $\mathtt{Y}$ is filled with the reciprocal of the first column of the file
      ``\texttt{cars.csv}''. 
\item $\mathtt{xMean}$ is the mean value $\bar{x}$ of the independent variable $\mathbf{x}$.
\item $\mathtt{yMean}$ is the mean value $\bar{\mathbf{y}}$ of the dependent variable $\mathbf{y}$. 
\item The coefficient $\mathtt{theta1}$ is computed according to Equation \ref{eq:theta1}, which is repeated
      here for convenience:
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\vartheta_1 = \frac{\sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr) \cdot \bigl(y_i - \bar{\mathbf{y}}\bigr)}{
                        \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2}  
      $.
\item The coefficient $\mathtt{theta0}$ is computed according to Equation \ref{eq:theta0}, which reads
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\vartheta_0 = \bar{\mathbf{y}} - \vartheta_1 \cdot \bar{\mathbf{x}}$.
\item $\mathtt{TSS}$ is the \blue{total sum of squares} and is computed using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\mathtt{TSS} = \sum\limits_{i=1}^m \bigl(y_i - \bar{\mathbf{y}}\bigr)^2$.
\item $\mathtt{RSS}$ is the \blue{residual sum of squares} and is computed as
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds\mathtt{RSS} := \sum\limits_{i=1}^m \bigl(\vartheta_1 \cdot x_i + \vartheta_0 - y_i\bigr)^2$.
\item $\mathtt{R2}$ is the $\mathtt{R}^2$ statistic and measures the \blue{proportion of the explained variance}.
      It is computed using the formula
      \\[0.2cm]
      \hspace*{1.3cm}
      $\ds \mathtt{R}^2 = \frac{\mathtt{TSS} - \mathtt{RSS}}{\mathtt{TSS}}$.
\item The function $\mathtt{plot\_createCanvas}$ creates a $\mathtt{canvas}$ that is used for plotting.
\item The function $\mathtt{plot\_addBullets}$ plots data point onto the $\mathtt{canvas}$.
      The second argument to this function is a list of pairs of the form
      \\[0.2cm]
      \hspace*{1.3cm}
      $[ \pair(x_1,y_1), \cdots, \pair(x_m,y_m)]$
      \\[0.2cm]
      The pair $\pair(x_i,y_1)$ is then plotted as a blue circle with radius $2.0$ at the position
      $\pair(x_i, y_i)$.
\end{enumerate}
\myFig{table.stlx} shows the implementation of the class table and the function $\mathtt{readTable}$.  This figure is
only given for completeness.

\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    class Table(columnNames, types, data) {
        mColumnNames := columnNames;
        mTypes       := types;
        mData        := data;
      static {
          getColumnNames := [ ] |-> mColumnNames;
          getTypes       := [ ] |-> mTypes;
          getData        := [ ] |-> mData;
          getRow         := [r] |-> mData[r];
          getLength      := [ ] |-> #mData;
          
          head := procedure(limit := 10) {
              print(mColumnNames);
              print(mTypes);
              for (i in [1 .. limit]) {
                  print(mData[i]);
              }
          };
      }
    }
    readTable := procedure(fileName, types) {
        allData     := readFile(fileName);  // list of lines
        columnNames := split(allData[1], ',\s*');
        data        := [];
        for (i in [2 .. #allData]) {
            row       := split(allData[i], ',\s*');
            data[i-1] := [evalType(type, s) : [type, s] in types >< row];
        }
        return Table(columnNames, types, data);
    };
    evalType := procedure(type, s) {
        switch {
            case     type == "double": return double(s);
            case     type == "int"   : return int(s);
            case     type == "string": return s;
            default: abort("unknown type $type$ in evalType");
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{The class $\mathtt{Table}$.}
\label{fig:table.stlx}
\end{figure}


\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    test := procedure() {
        types := ["double", "int", "double", "double", "double", "double", "int", "string"];
        for (varName in ["displacement", "cyl", "hp", "weight", "acc", "year"]) {
            R2 := simple_linear_regression("cars.csv", types, varName);
            print("The explained variance for $varName$ vs fuel consumption is $R2$."); 
        }
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{Calling the procedure $\mathtt{simple\_linear\_regression}$.}
\label{fig:simple-linear-regression.stlx:test}
\end{figure}

\myFig{simple-linear-regression.stlx:test} shows how to call the procedure
$\mathtt{simple\_linear\_regression}$.  We need to define the types of the various columns that are present in
the file ``\texttt{cars.csv}''.  Next, for all those column names present in this file, we use this variable to
compute the explained variance.  The resulting values are shown in Table \ref{tab:explained-variance}.  It
seems that, given the data in the file ``\texttt{cars.csv}'', the best indicator for the fuel consumption is
the $\mathtt{weight}$ of a car.  The $\mathtt{displacement}$, the power $\mathtt{hp}$ of an engine, and the
number of cylinders $\mathtt{cyl}$ are also
good predictors.  But notice that the $\mathtt{weight}$ is the real cause of fuel consumption:  If a car
has a big weight, it will also need a more powerful engine.  Hence the variable $\mathtt{hp}$ is correlated
with the variable $\mathtt{weight}$ and will therefore also provide a reasonable explanation of the fuel
consumption, although the high engine power is  not the most important cause of the fuel consumption.


\begin{table}
  \centering
  \begin{tabular}{|l|r|}
  \hline
  dependent variable      & explained variance   \\
  \hline
  \hline
  $\mathtt{displacement}$ & 0.75                 \\
  \hline
  $\mathtt{cyl}$          & 0.70                 \\
  \hline
  $\mathtt{hp}$           & 0.73                 \\
  \hline
  $\mathtt{weight}$       & 0.78                 \\
  \hline
  $\mathtt{acc}$          & 0.21                 \\
  \hline
  $\mathtt{year}$         & 0.31                 \\
  \hline
  \end{tabular}
  \caption[explained variance]{Explained variance for various dependent variables.}
  \label{tab:explained-variance}
\end{table}
\pagebreak


\subsection{Testing the Statistical Significance}
In this section we answer the question how to access the 
\href{https://en.wikipedia.org/wiki/Statistical_significance}{statistical significance} of our
results.  In the case of simple linear regression, the
\href{https://en.wikipedia.org/wiki/Null_hypothesis}{null hypothesis} is given as follows: 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{H}_0$: There is no relationship between the variables $x$ and $y$.
\\[0.2cm]
In order to compute the probability that the null hypothesis is true, we have to compute the
\emph{\color{blue} $t$-statistic}, which is given by the following formula:
\begin{equation}
  \label{eq:t-statistics}
  t := |\vartheta_1| \cdot \sqrt{\frac{\;(m-2)\;}{\mathtt{RSS}} \cdot \sum\limits_{i=1}^m \bigl(x_i - \bar{\mathbf{x}}\bigr)^2\; }.
\end{equation}
The $t$ statistics is distributed according to 
\href{https://en.wikipedia.org/wiki/Student%27s_t-distribution}{Student's $t$-distribution} 
with $m-2$ degrees of freedom.  In the upcomming version of \textsc{SetlX}, a preview of which is already available at
\\[0.2cm]
\hspace*{1.3cm}
\href{https://github.com/jonasunigithub/setlX}{\texttt{https://github.com/jonasunigithub/setlX}},
\\[0.2cm]
the cumulative Student's $t$-distribution can be computed
via the function
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{stat\_studentCDF}(t, \nu)$,
\\[0.2cm]
where $t$ is the value of the $t$-statistic and $\nu$ is the number of degrees of freedom.  If we substitute
$\nu := m - 2$, then the expression 
\\[0.2cm]
\hspace*{1.3cm}
$1 - \mathtt{stat\_studentCDF}(t, \nu)$,
\\[0.2cm]
 computes the probability that the null hypothesis is true.  This probability
is also known as the \blue{$p$-value}.  If the $p$-value is small, e.g.~less than $0.01$, then the null
hypothesis is refuted and we can conclude that there is some relationship between $x$ and $y$.  In the example
previously discussed, i.e.~the example relating the fuel consumption of a car to various other parameters, the
$p$-values are all very small, i.e.~less than $2 \cdot 10^{-16}$.  This is due to the fact that we have had enough data at
our disposal:  As a rule of thumb it can be said that once we have more than 30 pairs $\pair(x_i, y_i)$ and there
is indeed a relationship between $x$ and $y$, then simple linear regression will detect this relationship.

\section{General Linear Regression}
In practise, it is rarely the case that a given observed variable $y$ only depends on a single variable $x$.
To take the example of the fuel consumption of a car further, in general we would expect that the fuel consumption
of a car does depend not only on the mass of the car but is also related to the other parameters.  
To this end, we present the theory of   
\href{https://en.wikipedia.org/wiki/Linear_regression}{general linear regression}.
In a \blue{general regression problem} we are given a list of $m$ pairs of the form $\langle\mathbf{x}^{(i)}, y^{(i)} \rangle$ 
where $\mathbf{x}^{(i)} \in \mathbb{R}^p$ and $y^{(i)} \in \mathbb{R}$ for all $i \in \{1,\cdots,m\}$.  The
number $p$ is called the number of \blue{features}, while the pairs are called the \blue{training examples}.
Our goal is to compute a function 
\\[0.2cm]
\hspace*{1.3cm}
$F:\mathbb{R}^p \rightarrow \mathbb{R}$
\\[0.2cm]  
such that $F\bigl(\mathbf{x}^{(i)}\bigr)$ approximates  $y^{(i)}$ as precisely as posssible
for all $i\in\{1,\cdots,m\}$, i.e.~we want to have
\\[0.2cm]
\hspace*{1.3cm}
$\forall i\in\{1,\cdots,m\}:F\bigl(\mathbf{x}^{(i)}\bigr) \approx y^{(i)}$.
\\[0.2cm]
In order to make the notation $F\bigl(\mathbf{x}^{(i)}\bigr) \approx y^{(i)}$ more precise, we
define the \blue{mean squared error} 
\begin{equation}
  \label{eq:squared-error-1}
  \mathtt{MSE} := \frac{1}{m-1} \cdot \sum\limits_{i=1}^{m} \Bigl(F\bigl(\mathbf{x}^{(i)}\bigr) - y^{(i)}\Bigr)^2. 
\end{equation}
Then, given the list of training examples $[\langle \mathbf{x}^{(1)}, y^{(1)} \rangle, \cdots, \langle
\mathbf{x}^{n}, y^{(n)} \rangle]$, our goal is to minimize $\mathtt{MSE}$.  
In order to proceed, we need to have a model for the function $F$.  The simplest model is a linear
model, i.e.~we assume that $F$ is given as 
\\[0.2cm]
\hspace*{1.3cm}
$\ds F(\mathbf{x}) = \sum\limits_{j=1}^p w_j \cdot x_j + b = \mathbf{x}^T \cdot \mathbf{w} + b$ \quad where $\mathbf{w} \in \mathbb{R}^p$ and $b\in\mathbb{R}$.
\\[0.2cm]
Here, the expression $\mathbf{x}^T \cdot \mathbf{w}$ denotes the matrix product of the vector
$\mathbf{x}^T$, which is viewed as a $1$-by-$m$ matrix, and the vector $\mathbf{w}$.  Alternatively, this
expression could be interpreted as the dot product of the vector $\mathbf{x}$ and the vector $\mathbf{w}$.
At this point you might wonder why it is useful to introduce matrix notation here.  The reason is
that this notation shortens the formula and, furthermore, is more efficient to implement since most
programming languages used in machine learning have special library support for matrix operations.  
Provided the computer is equipped with a graphic card,  some
programming languages are even able to delegate matrix operations to the graphic unit.  This results in a
considerable speed up.

The definition of $F$ given above is the model used in
\href{https://en.wikipedia.org/wiki/Linear_regression}{linear regression}. 
Here, $\mathbf{w}$ is called the \blue{\color{blue}weight vector} and $b$ is called the \blue{\color{blue}bias}.  It turns
out that the notation can be simplified if we extend the $p$-dimensional feature vector $\mathbf{x}$ to an
$p+1$-dimensional vector $\mathbf{x}'$ such that
\\[0.2cm]
\hspace*{1.3cm}
$x_j' := x_j$ \quad for all $j\in\{1,\cdots,p\}$ \quad and \quad $x_{m+1}' := 1$.
\\[0.2cm]
To put it in words, the vector $\mathbf{x}'$ results from the vector $\mathbf{x}$ by appending the number $1$:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}' = \langle x_1, \cdots, x_p, 1 \rangle^T$ \quad where $\langle x_1, \cdots, x_p \rangle = \mathbf{x}^T$.
\\[0.2cm]
Furthermore, we define 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}' := \langle w_1, \cdots, w_p, b \rangle^T$ \quad where $\langle w_1, \cdots, w_p \rangle = \mathbf{w}^T$.
\\[0.2cm]
Then we have
\\[0.2cm]
\hspace*{1.3cm}
$F(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b = \mathbf{w}' \cdot \mathbf{x}'$.
\\[0.2cm]
Hence, the bias has been incorporated into the weight vector at the cost of appending a $1$ at the
input vector.  As we want to use this simplification, from now on we assume that the input vectors
$\mathbf{x}^{(i)}$ have all been extended so that there last component is $1$.  Using this
assumption,  we define the
function $F$ as
\\[0.2cm]
\hspace*{1.3cm}
$F(\mathbf{x}) := \mathbf{x}^T \cdot \mathbf{w}$.
\\[0.2cm]
Now equation (\ref{eq:squared-error-1}) can be rewritten as follows:
\begin{equation}
  \label{eq:squared-error-2}
  \mathtt{MSE}(\mathbf{w}) = \frac{1}{m-1} \cdot \sum\limits_{i=1}^n \Bigl(\bigl(\mathbf{x}^{(i)})^T \cdot \mathbf{w}  - y^{(i)}\Bigr)^2.
\end{equation}
Our aim is to rewrite the sum appearing in this equation as a scalar product of a vector with
itself.  To this end, we first define the vector $\mathbf{y}$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{y} := \langle y^{(1)}, \cdots, y^{(m)} \rangle^T$.
\\[0.2cm]
Note that $\mathbf{y} \in \mathbb{R}^m$ since it has a component for all of the $m$ training
examples.  Next, we define the matrix $X$ as follows:
\\[0.2cm]
\hspace*{1.3cm}
$X := \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^T  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^T
  \end{array}
  \right)   
$
\\[0.2cm]
Defined this way, the row vectors of the matrix $X$ are the vectors $\mathbf{x}^{(i)}$ transposed.
Now we have the following:
\\[0.2cm]
\hspace*{1.3cm}
$X \cdot \mathbf{w} - \mathbf{y} = \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^T  \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^T
  \end{array}
  \right) \cdot \mathbf{w} - \mathbf{y} = \left(
  \begin{array}{c}
    \bigl(\mathbf{x}^{(1)}\bigr)^T \cdot \mathbf{w} - y_1 \\
    \vdots                         \\
    \bigl(\mathbf{x}^{(m)}\bigr)^T \cdot \mathbf{w} - y_m
  \end{array}
  \right)
$
\\[0.2cm]
Taking the square of the vector $X \cdot \mathbf{w} - \mathbf{y}$ we discover that
we can rewrite equation (\ref{eq:squared-error-2}) as follows:
\begin{equation}
  \label{eq:squared-error-3}
  \mathtt{MSE}(\mathbf{w}) = \frac{1}{m-1} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr)^T \cdot 
                                            \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr).
\end{equation}

\subsection{Some Useful Gradients}
In the last section, we have computed the squared error $E(\mathbf{w})$ using equation
(\ref{eq:squared-error-3}).  Our goal is to minimize the $E(\mathbf{w})$ by choosing the weight
vector $\mathbf{w}$ appropriately.  A necessary condition for $E(\mathbf{w})$ to be minimal is 
\\[0.2cm]
\hspace*{1.3cm}
$\nabla E(\mathbf{w}) = \mathbf{0}$,
\\[0.2cm]
i.e.~the gradient of $E(\mathbf{w})$ needs to be zero.  In order to prepare for the computation of
$\nabla E(\mathbf{w})$, we first compute the gradient of two simpler functions.

\subsubsection{Computing the Gradient of $f(\mathbf{x}) = \mathbf{x}^T \cdot C \cdot \mathbf{x}$}
Suppose the function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$f(\mathbf{x}) := \mathbf{x}^T \cdot C \cdot \mathbf{x}$ \quad where $C \in \mathbb{R}^{n \times n}$.
\\[0.2cm]
If we write the matrix $C$ as $C = (c_{i,j})_{i=1,\cdots,n \atop j=1,\cdots,n}$ and the vector
$\mathbf{x}$ as $\mathbf{x} = \langle x_1, \cdots, x_n \rangle^T$,  then $f(\mathbf{x})$ can be
computed as follows:
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(\mathbf{x}) = \sum\limits_{i=1}^n x_i \cdot \sum\limits_{j=1}^n c_{i,j} \cdot x_j 
                   = \sum\limits_{i=1}^n \sum\limits_{j=1}^n x_i \cdot c_{i,j} \cdot x_j
$.
\\[0.2cm]
We compute the partial derivative of $f$ with respect to $x_k$ and use the product rule together with the
definition of the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta} $\delta_{i,j}$, which
is defined as $1$ if $i = j$ and as $0$ otherwise:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
\ds \frac{\partial f}{\partial x_k} & = &
\ds \sum\limits_{i=1}^n \sum\limits_{j=1}^n \Bigl(
    \frac{\partial x_i}{\partial x_k} \cdot c_{i,j} \cdot x_j + x_i \cdot c_{i,j} \cdot \frac{\partial x_j}{\partial x_k}
    \Bigr) \\[0.5cm]
& = &
\ds \sum\limits_{i=1}^n \sum\limits_{j=1}^n \Bigl(
    \delta_{i,k} \cdot c_{i,j} \cdot x_j + x_i \cdot c_{i,j} \cdot \delta_{j,k} \Bigr) \\[0.5cm]
& = &
\ds \sum\limits_{j=1}^n c_{k,j} \cdot x_j + \sum\limits_{i=1}^n x_i \cdot c_{i,k} \\[0.5cm]
& = &
  \bigl(C \cdot \mathbf{x}\bigr)_k + \bigl(C^\textrm{T} \cdot \mathbf{x}\bigr)_k
\end{array}
$
\\[0.2cm]
Hence we have shown that 
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f(\mathbf{x}) = (C + C^\textrm{T}) \cdot \mathbf{x}$.
\\[0.2cm]
If the matrix $C$ is symmetric, i.e.~if $C = C^\textrm{T}$, this simplifies to
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla f(\mathbf{x}) = 2 \cdot C \cdot \mathbf{x}$.
\\[0.2cm]
Next, if the function $g: \mathbb{R}^n \rightarrow \mathbb{R}$ is defined as 
\\[0.2cm]
\hspace*{1.3cm}
$g(\mathbf{x}) := \mathbf{b}^\textrm{T} \cdot A \cdot \mathbf{x}$, \quad where $\mathbf{b} \in \mathbb{R}^n$ and $A \in \mathbb{R}^{n \times n}$,
\\[0.2cm]
then a similar calculation shows that
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla g(\mathbf{x}) = A^\textrm{T} \cdot \mathbf{b}$.

\exercise
Prove this equation.

\section{Deriving the Normal Equation}
Next, we derive the so called \blue{normal equation} for linear regression.  To this end, we first
expand the product in equation (\ref{eq:squared-error-3}):
\\[0.2cm]
\hspace*{0.3cm}
$
\begin{array}[t]{lcll}
 \mathtt{MSE}(\mathbf{w}) & = & 
 \ds \frac{1}{m-1} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr)^\textrm{T} \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr) 
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\textrm{T} \cdot X^\textrm{T} - \textbf{y}^\textrm{T}\bigr) \cdot \bigl(X \cdot \mathbf{w} - \textbf{y}\bigr) 
 & \mbox{since $(A \cdot B)^\textrm{T} = B^\textrm{T} \cdot A^\textrm{T}$}
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot X \cdot \mathbf{w} 
                             - \textbf{y}^\textrm{T} \cdot X \cdot \mathbf{w} 
                             - \mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot \mathbf{y}
                             + \mathbf{y}^\textrm{T} \cdot \mathbf{y}
                       \bigr)
 \\[0.5cm]
 & = & 
 \ds \frac{1}{m-1} \cdot \bigl(\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot X \cdot \mathbf{w} 
                             - 2 \cdot \textbf{y}^\textrm{T} \cdot X \cdot \mathbf{w} 
                             + \mathbf{y}^\textrm{T} \cdot \mathbf{y}
                       \bigr)
 & \mbox{since $\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot \mathbf{y} = \textbf{y}^\textrm{T} \cdot X \cdot \mathbf{w}$}
\end{array}
$
\\[0.2cm]
The fact that 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot \mathbf{y} = \textbf{y}^\textrm{T} \cdot X \cdot \mathbf{w}$
\\[0.2cm]
might not be immediately obvious.  It follows from two facts:
\begin{enumerate}
\item For two matrices $A$ and $B$ such that the matrix product $A \cdot B$ is defined we have 
      \\[0.2cm]
      \hspace*{1.3cm}
      $(A \cdot B)^\mathrm{T} = B^\mathrm{T} \cdot A^\mathrm{T}$.
\item The matrix product $\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot \mathbf{y}$ is a real number.  The transpose $r^\textrm{T}$ of a real number $r$ is the number
      itself, i.e.~$r^\textrm{T} = r$ for all $r \in \mathbb{R}$.  Therefore, we have
      \\[0.2cm]
      \hspace*{1.3cm}
      $\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot \mathbf{y} = 
\bigl(\mathbf{w}^\textrm{T} \cdot X^\textrm{T} \cdot \mathbf{y}\bigr)^\textrm{T} =
\mathbf{y}^\textrm{T} \cdot X \cdot \mathbf{w}
$.
\end{enumerate}
Hence we have shown that
\begin{equation}
  \label{eq:squared-error-4}
  \mathtt{MSE}(\mathbf{w}) = \ds \frac{1}{m-1} \cdot \Bigl(\mathbf{w}^\textrm{T} \cdot \bigl(X^\textrm{T} \cdot X\bigr) \cdot \mathbf{w} 
                                             - 2 \cdot \textbf{y}^\textrm{T} \cdot X \cdot \mathbf{w} 
                                             + \mathbf{y}^\textrm{T} \cdot \mathbf{y}
                                        \Bigr)
\end{equation}
holds.  The matrix $X^\textrm{T} \cdot X$ used in the first term is symmetric because
\\[0.2cm]
\hspace*{1.3cm}
$\bigl(X^\textrm{T} \cdot X\bigr)^\textrm{T} = X^\textrm{T} \cdot \bigl(X^\textrm{T}\bigr)^\textrm{T} = X^\textrm{T} \cdot X$.
\\[0.2cm]
Using the results from the previous section we can now compute the gradient of $\mathtt{MSE}(\mathbf{w})$ with respect to
$\mathbf{w}$.  The result is
\\[0.2cm]
\hspace*{1.3cm}
$\ds \nabla \mathtt{MSE}(\mathbf{w}) = \frac{2}{m-1} \cdot \Bigl(X^\textrm{T} \cdot X \cdot \mathbf{w} - X^\textrm{T} \cdot \mathbf{y}\Bigr)$.
\\[0.2cm]
If the squared error $\mathtt{MSE}(\mathbf{w})$ has a minimum for the weights $\mathbf{w}$, then we must have
\\[0.2cm]
\hspace*{1.3cm}
$\nabla \mathtt{MSE}(\mathbf{w}) = \mathbf{0}$.
\\[0.2cm]
This leads to the equation
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{2}{m-1} \cdot \Bigl(X^\textrm{T} \cdot X \cdot \mathbf{w} - X^\textrm{T} \cdot \mathbf{y}\Bigr) = \mathbf{0}$.
\\[0.2cm]
This equation can be rewritten as
\begin{equation}
  \label{eq:normal-equation}
  \bigl(X^\textrm{T} \cdot X\bigr) \cdot \mathbf{w} = X^\textrm{T} \cdot \mathbf{y}.          
\end{equation}
This equation is called the \blue{normal equation}.
Now, if the matrix $X^\textrm{T} \cdot X$ is invertible, then this equation can be rewritten as
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{yellow}{\framebox{
$\mathbf{w} = \bigl(X^\textrm{T} \cdot X)^{-1} \cdot X^\textrm{T} \cdot \mathbf{y}$.
}}}}
\\[0.2cm]
In this case, we define 
\\[0.2cm]
\hspace*{1.3cm}
$X^+ := \bigl(X^\textrm{T} \cdot X)^{-1} \cdot X^\textrm{T}$.
\\[0.2cm]
The expression $X^+$ is called the 
\href{https://en.wikipedia.org/wiki/Moore–Penrose_pseudoinverse}{Moore-Penrose pseudoinverse} of the matrix $X$.
We can understand in what sense $X^+$ is an inverse of $X$ by noting that if $X$ itself were invertible, we could solve
the equation
\\[0.2cm]
\hspace*{1.3cm}
$X \cdot \mathbf{w} = \mathbf{y}$
\\[0.2cm]
by defining
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} := X^{-1} \cdot \mathbf{y}$.
\\[0.2cm]
In that case, the squared error would be zero.
In general, $X$ will not be invertible for the simple reason that the matrix $X$ is an $m \times p$ matrix and hence is
not even a square matrix if $m \not= p$.  In practical applications of linear regression the number $m$ of training
examples is bigger than the dimension $p$ of the feature vectors $\mathbf{x}_i$ that make up the training examples.  Hence there
is no hope of $X$ being invertible.  The next best thing is then to minimize the squared error.  To do this, we take the
previous equation for the weight vector $\mathbf{w}$ and replace $X^{-1}$ with the pseudoinverse of $X$.  This gives the
equation 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{w} = \bigl(X^\textrm{T} \cdot X)^{-1} \cdot X^\textrm{T} \cdot \mathbf{y}$
\\[0.2cm]
derived previously.

At this point you might ask whether the matrix $X^\textrm{T} \cdot X$ is always invertible.  The short answer is yes, provided the
regression problem is well posed.  By this I mean that there are much more training examples than there are different
weights.  After all, if you have only 10 training examples but want to determine one hundred different weights,
then it is obvious that the problem is not well posed.  Now if you have enough training examples you have to be
sure that the training examples are independent of each other.  For example, lets say you have three training examples 
$\langle\mathbf{x}_1, y^{(1)}\rangle$, $\langle\mathbf{x}_2, y^{(2)}\rangle$, and $\langle\mathbf{x}_3, y^{(3)}\rangle$ and
you decide that you define a fourth training example $\langle\mathbf{x_4}, y^{(4)}\rangle$ as
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{x}_4 := \mathbf{x}_1 + \mathbf{x}_2 + \mathbf{x}_3$ \quad and \quad 
$y^{(4)} := y^{(1)} + y^{(2)} + y^{(3)}$,
\\[0.2cm]
then you have not generated any new information and the fourth training example will not help in making the matrix 
$X^\textrm{T} \cdot X$ invertible.

\subsection{Testing the Statistical Significance}
The $\mathrm{F}$-statistic is defined according to the following formula:
\begin{equation}
  \label{eq:F-statistic}
  \mathrm{F} = \frac{\mathtt{TSS} - \mathtt{RSS}}{\mathtt{RSS}} \cdot \frac{m - p - 1}{p}
\end{equation}
The $\mathtt{F}$-statistic is distributed according to the
\href{https://en.wikipedia.org/wiki/F-distribution}{Fisher-Snedecor-distribution} with $m$ degrees of freedom
in the nominator and $m - p - 1$ degrees of freedom in the denominator. 

\subsection{Implementation}
\begin{figure}[!ht]
\centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    run_linear_regression := procedure(fileName, types) {
        csv    := readTable(fileName, types);
        data   := csv.getData();
        number := #data;
        dmnsn  := #data[1];    
        yList  := [];
        xList  := [];
        for (i in [1 .. number]) {
            yList[i] := 1 / data[i][1];
            xList[i] := la_vector([1.0] + data[i][2..-2]);
        }
        X := la_matrix(xList);
        y := la_vector(yList);
        w := la_solve(X! * X, X! * y);
        d := X * w - y;
        E := +/ [ abs(d[i]) : i in [1 .. number] ] / number;
        return [w, E];
    };
\end{Verbatim}
\vspace*{-0.3cm}
\caption{General linear regression.}
\label{fig:linear-regression.stlx}
\end{figure}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "artificial-intelligence"
%%% End:
